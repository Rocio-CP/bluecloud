{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aff99363",
   "metadata": {},
   "source": [
    "First, let's import some libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccddf96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "from urllib.error import HTTPError\n",
    "from urllib.error import URLError\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "from erddapy import ERDDAP\n",
    "from erddapy.doc_helpers import show_iframe\n",
    "from erddapy import servers\n",
    "\n",
    "# Set to display full length of rows\n",
    "pd.set_option('max_colwidth',None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bf3b7a",
   "metadata": {},
   "source": [
    "# Explore availability of data using ERDDAP\n",
    "## What is ERDDAP?\n",
    "From (https://coastwatch.pfeg.noaa.gov/erddap/information.html): *ERDDAP is a data server that gives you a simple, consistent way to download subsets of scientific datasets in common file formats and make graphs and maps.*\n",
    "It allows to select and download a subset of data, removing the need to download unnecesarily large files. You can download the data in your preferred format, regardless of the format of origin. It standardizes the variable names and units of position (latitude, longitude, altitude/depth) and time; it is particularly useful for time variables.\n",
    "\n",
    "The two most common ERDDAP protocols are *tabledap* and *griddap*. Tabledap is for table-like data, like individual geolocated observations. Griddap is for gridded datasets, for example gridded climatologies or model outputs.\n",
    "\n",
    "erddapy is a python package that helps building the ERDDAP query URLs. For more information, the documentation can be found here: https://ioos.github.io/erddapy/\n",
    "\n",
    "## Find datasets in ERDDAP\n",
    "If you do not know any ERDDAP servers, you can search text search across 56 predefined instances here: http://erddap.com/. In addition, you can add servers you know of, or filter servers.\n",
    "\n",
    "For a in-notebook solution, the cell below will retrieve a list of erddap servers and the datasets that have carbon variables, using their CF standard names. There are several standard vocabularies used in the natural sciences and the marine environment; for example SDN codes will appear in our searches. However CF is the most international and the attribute name *standard_name* is standardized, too, which contributes to interoperability.\n",
    "\n",
    "**WARNING** it takes quite some time, approximately 15 minutes; the website is a lot quicker. Therefore a `exploreerddap` switch is in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9b7f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "exploreerddap=False\n",
    "\n",
    "if exploreerddap:\n",
    "    stdnames=['surface_partial_pressure_of_carbon_dioxide_in_sea_water',\n",
    "    'fugacity_of_carbon_dioxide_in_sea_water',\n",
    "    'mole_concentration_of_dissolved_inorganic_carbon_in_sea_water',\n",
    "    'moles_of_dissolved_inorganic_carbon_per_unit_mass_in_sea_water',\n",
    "    'sea_water_ph_reported_on_total_scale',\n",
    "    'sea_water_alkalinity_expressed_as_mole_equivalent']\n",
    "    starttime=time.time()\n",
    "    for s in servers:\n",
    "        e=ERDDAP(s)\n",
    "        for v in stdnames:\n",
    "            kw= {\"standard_name\": v}\n",
    "            search_url = e.get_search_url(response=\"csv\", **kw)\n",
    "            try:\n",
    "                search=pd.read_csv(search_url)\n",
    "            except HTTPError:\n",
    "                pass\n",
    "                print(\"No\", stdname, \"data in the server\", s)\n",
    "            except URLError:\n",
    "                pass\n",
    "                print(s,\"throws an URL error\")\n",
    "            else:\n",
    "                print(s,\"has\", {len(set(search[\"tabledap\"].dropna()))},\"tabledap datasets with\",v)\n",
    "                print(search[[\"Dataset ID\",\"Title\"]])\n",
    "                if len(set(search[\"griddap\"].dropna())) > 0:\n",
    "                    print(s,\"has\", {len(set(search[\"griddap\"].dropna()))},\"griddap datasets with\",v)\n",
    "\n",
    "\n",
    "\n",
    "    endtime=time.time()-starttime\n",
    "    print(\"\\n Search took\", endtime/60,\"minutes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d63d60",
   "metadata": {},
   "source": [
    "# pH data from two ERDDAP servers: EMODnet and IFREMER\n",
    "For this exercise, we will search for pH data from two european sources: EMODnet and Ifremer. pH is the variable that is measured the most often, and has one of the longest time and space coverages of all the inorganic carbon variables.\n",
    "\n",
    "## List datasets that contain pH in EMODnet\n",
    "Get list of table-like **datasets** that contain pH (not gridded). We search by using the *standard_name* attribute and its Climate and Forecast (CF) name.\n",
    "\n",
    "**e** (or **i** for the IFREMER server) is a python object with multiple methods. The ones we will use are:\n",
    "* response: the format of the response. We will use .csv and read those to pandas dataframes, but many others are available (.nc, .json, .html, .mat, .tsv ...)\n",
    "* get_* : returns a valid ERDDAP URL. **get_search_url** provides a list of datasets according the criteria established. **get_info_url** provides information about a particular dataset\n",
    "* constraints: search and download constraints. Time, position, values of variables and/ or attributes\n",
    "* dataset_id: to set which dataset we will explorte\n",
    "* variables: set the variables to download if you do not wish to download the full dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0039178",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = ERDDAP(server=\"EMODNET\", protocol='tabledap')\n",
    "e.response= \"csv\"\n",
    "search_url = e.get_search_url(**{\n",
    "    \"standard_name\": \"sea_water_ph_reported_on_total_scale\"})\n",
    "e_search = pd.read_csv(search_url)\n",
    "display(e_search[[\"Title\",\"Dataset ID\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd94d414",
   "metadata": {},
   "source": [
    "We are interested in all the pH measurements, not only their minimum/maximum values, which leaves the `MINMAXMEAN` dataset out. We will pick the profiles dataset first.\n",
    "## Metadata\n",
    "Before downloading the data itself, we will get some further information about the dataset **EP_ERD_INT_PHPH_AL_PR_NRT**:\n",
    "* From the ID, we know that the data is NRT, so it will have gone through some, but not full QC (adjustments, calibrations, etc\n",
    "* List the variables available in the dataset. We may be interested in all, or only some of them\n",
    "* List the attributes of those variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006415e0",
   "metadata": {},
   "source": [
    "### List of variables available\n",
    "When retrieving the dataset information using **get_info_url**, we get the full list of attributes, both global (`Variable Name NC_GLOBAL`) and per variable. In this case, that results in >200 lines. It provides also information about the type and the value of the attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c652650",
   "metadata": {},
   "outputs": [],
   "source": [
    "e.dataset_id=\"EP_ERD_INT_PHPH_AL_PR_NRT\"\n",
    "e_info = pd.read_csv(e.get_info_url())\n",
    "display(e_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1756c326",
   "metadata": {},
   "source": [
    "To get only the list of variable names, we can subset by `Row Type = variable`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e022f536",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\", \".join(e_info.loc[e_info[\"Row Type\"] == \"variable\", \"Variable Name\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22a1ab4",
   "metadata": {},
   "source": [
    "From the list we see the variable names as EMODnet stores them, information we need to retrieve their attributes. Let's check the time range available and the attributes for the variables `PHPH`and `PHPH_QC`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1062cf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time coverage\n",
    "print('Time coverage for the dataset is from', e_info.loc[e_info[\"Attribute Name\"] == \"time_coverage_start\", \"Value\"].item(),\n",
    "       'to' , e_info.loc[e_info[\"Attribute Name\"] == \"time_coverage_end\", \"Value\"].item())\n",
    "# pH attributes\n",
    "print()\n",
    "print('The attributes of the variable PHPH (pH) are')\n",
    "display(e_info.loc[e_info[\"Variable Name\"] == \"PHPH\", :])\n",
    "print('The attributes of the variable PHPH_QC (pH quality control flag) are')\n",
    "display(e_info.loc[e_info[\"Variable Name\"] == \"PHPH_QC\", :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4e5d1b",
   "metadata": {},
   "source": [
    "Information about the type of platform that recorded those datapoints is interesting, too, specially if we will compare with other sources of pH data. EMODnet has their own codes for platform type, stored in the variable `EP_PLATFORM_TYPE`. `platform_code` and `wmo_platform_code` can be of interest, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ab1e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(e_info.loc[e_info[\"Variable Name\"] == \"EP_PLATFORM_TYPE\", :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841f70cf",
   "metadata": {},
   "source": [
    "Position and time variables have a particular attribute: `axis`. For geolocated variables, latitude and longitude are usually `Y` and `X` axis, respectively. For ocean data `Z` variables can be depth and/or pressure. Let's see which are the axis variables for EMODnet pH profile data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c282dfdb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(e_info.loc[e_info[\"Attribute Name\"] == \"axis\", [\"Variable Name\", \"Value\"]])\n",
    "# To get them as list\n",
    "e_varaxisname=e.get_var_by_attr(axis=lambda v: v in[\"X\",\"Y\",\"Z\",\"T\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9c86f3",
   "metadata": {},
   "source": [
    "## pH data from IFREMER ERDDAP servers\n",
    "We will now go through similar steps, to find and retrieve data from the IFREMER ERDDAP server (http://www.ifremer.fr/erddap/).\n",
    "They also use the attribute *cf_standard_name* in addition to *standard name*, which returns more datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc95efde",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = ERDDAP(server=\"IFREMER\", protocol='tabledap')\n",
    "search_url = i.get_search_url(response=\"csv\", **{\n",
    "    \"standard_name\": \"sea_water_ph_reported_on_total_scale\", # they use cf_standard\n",
    "    })\n",
    "i_search = pd.read_csv(search_url)\n",
    "display(i_search[[\"Title\",\"Dataset ID\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a3be67",
   "metadata": {},
   "source": [
    "Let's explore the BGC Argo dataset. Datasets with an automatic data flow usually have a lot of variables (in this particular case, in the hundreds!), because they record and distribute not only the final, calculated variables, but also a lot of device diagnostics and the variables that are used to do data reduction to the final variables. We will see an example in pH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b6ef36",
   "metadata": {},
   "outputs": [],
   "source": [
    "i.dataset_id=\"ArgoFloats-synthetic-BGC\"\n",
    "i.response=\"csv\"\n",
    "\n",
    "i_info = pd.read_csv(i.get_info_url())\n",
    "print(i.dataset_id,\"contains\",\n",
    "      str(i_info.loc[i_info[\"Row Type\"] == \"variable\", \"Variable Name\"].shape[0]),\"variables.\")\n",
    "\n",
    "display(i_info.loc[(i_info[\"Attribute Name\"] == \"long_name\") & \n",
    "                         (i_info[\"Variable Name\"].str.contains('(?:^|_)ph(?:_|$)'))\n",
    "                         , [\"Variable Name\", \"Value\"]])\n",
    "\n",
    "print(i.get_var_by_attr(standard_name=\"sea_water_ph_reported_on_total_scale\"),\n",
    "     \"is the variable with the standard_name attribute\")\n",
    "print(\"\\n\")\n",
    "print('Time coverage for the dataset is from', i_info.loc[i_info[\"Attribute Name\"] == \"time_coverage_start\", \"Value\"].item(),\n",
    "       'to' , i_info.loc[i_info[\"Attribute Name\"] == \"time_coverage_end\", \"Value\"].item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08390f7",
   "metadata": {},
   "source": [
    "We see that BGC-Argo distributes values of voltages, current, and sensor temperatures, which in themselves are not of scientific interest, but are necessary to calculate pH in situ values. They provide pH in three different scales (total, free and sewater scales); we will pick the total scale. \n",
    "\n",
    "Two variables are candidate to become the pH values we want to explore: `ph_in_situ_total` and `ph_in_situ_total_adjusted`, and their corresponding QC flag variables. Data measured by Argo floats may need certain adjustments (for example, due to drift. More information in [Bittig *et al.* (2019)]( https://doi.org/10.3389/fmars.2019.00502) ). The preservation of both \"raw\" and adjusted data is necessary in order to maintain reproducibility. \n",
    "\n",
    "We will keep both, too, and decide at a later stage what data to use, depending on the nature of the other data sources. For that, we need their attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec474b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for vph in i.get_var_by_attr(standard_name=\"sea_water_ph_reported_on_total_scale\"):\n",
    "    print(\"The attributes of the variable\", vph, \"(pH) are\")\n",
    "    display(i_info.loc[i_info[\"Variable Name\"] == vph, :])\n",
    "    print(\"The attributes of the variable\", vph+\"_qc\", \"(pH) are\")\n",
    "    display(i_info.loc[i_info[\"Variable Name\"] == vph+\"_qc\", :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af1526e",
   "metadata": {},
   "source": [
    "As with EMODnet, let's get some platform information. We know that the measuring devices are Argo floats, from the dataset name. In addition, we can explore what other platform metadata exists in the dataset. Both `platform_type` and `platform_number`are of interest. The Argo Users Manual (https://doi.org/10.13155/29825) contains information on the Argo reference tables mentioned in the attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f627fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(i_info.loc[(i_info[\"Variable Name\"].str.contains('(?:^|_)platform(?:_|$)'))\n",
    "                 , [\"Variable Name\", \"Attribute Name\",\"Value\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a2ffaf",
   "metadata": {},
   "source": [
    "Finally, let's check the axis variables. The main difference with EMODnet is that BGC-Argo does not use depth as Z-axis variables, but pressure and adjusted pressure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3cc0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(i_info.loc[i_info[\"Attribute Name\"] == \"axis\", [\"Variable Name\", \"Value\"]])\n",
    "# To get them as list\n",
    "i_varaxisname=i.get_var_by_attr(axis=lambda v: v in[\"X\",\"Y\",\"Z\",\"T\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071a5da7",
   "metadata": {},
   "source": [
    "# Download the data and export to a local file\n",
    "After exploring the variables available and their metadata, we will now proceed to download the data, filtering by a particular time frame and pH data of quality \"good\" and \"probably good\". Since the datasets contain NRT data, it is likely that many points have been flagged probably good, waiting for a manual QC.\n",
    "\n",
    "We set the variables to be downloaded (geolocation, time and pH, and some platform metadata) and the constraints in the erddap objects (**e** and **i**). Using the method `to_pandas` we can retrieve the data into a pandas dataframe. Other methods are `to_xarray` (particularly useful for gridded data) or `to_ncCF`.\n",
    "\n",
    "### EMODnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e09900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables to download\n",
    "e.variables=e_varaxisname + ['PHPH','PHPH_QC', 'EP_PLATFORM_TYPE', 'wmo_platform_code']\n",
    "\n",
    "# Constraints\n",
    "e.constraints= {\n",
    "    \"time>=\": \"2019-03-01T00:00:00Z\",\n",
    "    \"time<=\": \"2019-03-15T23:59:59Z\",\n",
    "    \"PHPH_QC>=\": 1, # good data\n",
    "    \"PHPH_QC<=\": 2} # probably good data\n",
    "\n",
    "dtype=object\n",
    "df_emodnet = e.to_pandas()\n",
    "\n",
    "# Print the variables retrieved\n",
    "print(\"The EMODNet dataframe variables retrieved are\", df_emodnet.columns.values)\n",
    "print(\"\\n\")\n",
    "# Size of the dataframe\n",
    "print(\"The size of the EMODnet dataframe is\", df_emodnet.shape[0],\"rows (data points) and\",df_emodnet.shape[1],\"columns (variables)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffe2f5a",
   "metadata": {},
   "source": [
    "### BGC-Argo (IFREMER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2551470c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of variables to download\n",
    "i.variables=i_varaxisname + ['ph_in_situ_total', 'ph_in_situ_total_qc','ph_in_situ_total_adjusted', 'ph_in_situ_total_adjusted_qc']\n",
    "\n",
    "i.constraints= {\n",
    "    \"time>=\": \"2019-03-01T00:00:00Z\",\n",
    "    \"time<=\": \"2019-03-15T23:59:59Z\",\n",
    "    \"ph_in_situ_total_qc=~\": \"(1|2)\"} # good data and probably good data\n",
    "        \n",
    "dtype=object\n",
    "df_ifremer = i.to_pandas()\n",
    "\n",
    "# Print the variables retrieved\n",
    "print(\"The BGC-Argo (IFREMER) dataframe variables retrieved are\", df_ifremer.columns.values)\n",
    "print(\"\\n\")\n",
    "# Size of the dataframe\n",
    "print(\"The size of the BGC-Argo (IFREMER) dataframe is\", df_ifremer.shape[0],\"rows (data points) and\",df_ifremer.shape[1],\"columns (variables)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c502ab5e",
   "metadata": {},
   "source": [
    "## Standardization of column names\n",
    "\n",
    "We will rename the common columns the same way, regardless of the data source. This will help when merging the dataframes. We also add a `SOURCE` variable that indicates where the data came from.\n",
    "For this, we create a dictionary, so we can change the variable names there, if necessary, instead of on each dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9424ac1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vardict ={'datevec':'DATEVECTOR',\n",
    "          'lat':'LATITUDE','lon':'LONGITUDE','dep':'DEPTH', 'pres':'PRESSURE',\n",
    "          'ph': 'pH_TS','phf': 'pH_FLAG'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873d8350",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emodnet.rename(\n",
    "    columns={'latitude (degrees_north)': vardict['lat'], 'longitude (degrees_east)': vardict['lon'],\n",
    "             'pres (dbar)': vardict['pres'], 'time (UTC)': vardict['datevec'],\n",
    "             'PHPH (1)': vardict['ph'], 'PHPH_QC (1)': vardict['phf']},\n",
    "        inplace=True)\n",
    "\n",
    "df_ifremer.rename(\n",
    "    columns={'latitude (degrees_north)': vardict['lat'], 'longitude (degrees_east)': vardict['lon'],\n",
    "             'pres (decibar)': vardict['pres'], 'time (UTC)': vardict['datevec'],\n",
    "             'ph_in_situ_total (dimensionless)': vardict['ph'], 'ph_in_situ_total_qc': vardict['phf']},\n",
    "        inplace=True)\n",
    "\n",
    "df_emodnet['SOURCE']='EMODnet pH profiles'\n",
    "df_ifremer['SOURCE']='BGC-Argo (IFREMER)'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af25f7ee",
   "metadata": {},
   "source": [
    "We will save the data downloaded as csv files, separate, in the same folder as the jupyter notebooks. The retrieval date at the beginning of the file name is important. These datasets update with certain regularity, and it is crucial to know when they were downloaded for reproducibility purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebef310d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dateforfile=datetime.now().strftime(\"%Y%m%d\")\n",
    "df_emodnet.to_csv(dateforfile+'_pH_data_ERDDAP_EMODNetpHProf.csv')\n",
    "df_ifremer.to_csv(dateforfile+'_pH_data_ERDDAP_BGCArgoIFREMER.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
