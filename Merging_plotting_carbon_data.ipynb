{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d733e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cartopy\n",
    "cartopy.config['data_dir'] = './maps'\n",
    "import cartopy.crs as ccrs\n",
    "from cartopy.feature import NaturalEarthFeature\n",
    "import cartopy.feature as feat\n",
    "\n",
    "# When plotting with Cartopy, a deprecation warning pops up \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "# It throws a SettingWithCopyWarning false positive\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b84152",
   "metadata": {},
   "source": [
    "### Import data frames: run the jupyter notebook or read the .csv files\n",
    "We will continue working on the df_emodnet and df_ifremer created by the previous Jupyter notebook. We can either choose to run the Jupyter notebook from here, or read the latest .csv file created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6549891",
   "metadata": {},
   "outputs": [],
   "source": [
    "switch=\"Jupyter\" # \"Jupyter\" or \"csv\"\n",
    "\n",
    "vardict ={'datevec':'TIME',\n",
    "      'lat':'LATITUDE','lon':'LONGITUDE','dep':'DEPTH', 'pres':'PRES',\n",
    "      'ph': 'PHPH','phf': 'PHPH_QC',\n",
    "      'wmo': 'WMO', 'platf': 'PLATFORM_NAME_OCEANOPS'}\n",
    "\n",
    "if switch == \"Jupyter\":\n",
    "    print(\"run carbon data from ERDDAP\")\n",
    "    %run \"./Carbon_data_from_ERDDAP.ipynb\"\n",
    "\n",
    "elif switch == \"csv\":\n",
    "    emodnetfiles=glob.glob('*EMODNet*.csv')\n",
    "    df_emodnet=pd.read_csv(max(emodnetfiles, key=os.path.getctime),dtype={vardict['wmo']:str})\n",
    "    bgcargofiles=glob.glob('*BGCArgo*.csv')\n",
    "    df_ifremer=pd.read_csv(max(bgcargofiles, key=os.path.getctime),dtype={vardict['wmo']:str})\n",
    "    \n",
    "\n",
    "else:\n",
    "    print(\"no valid input provided\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994b39ba",
   "metadata": {},
   "source": [
    "# Create a single dataframe\n",
    "Concatenate the dataframe, unify the series types, drop unnecessary information, and order in chronological order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0317135c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes=[df_emodnet,df_ifremer]\n",
    "df=pd.concat(dataframes)\n",
    "\n",
    "# Create a python datetime object in order to order and manipulate within the notebook\n",
    "df['dateobject']=pd.to_datetime(df[vardict['datevec']])\n",
    "df.sort_values(by=['dateobject',vardict['pres']],inplace=True)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e28cc4a",
   "metadata": {},
   "source": [
    "From a quick look at the dataframe, some data series may be dropped. `ph_in_situ_total_adjusted` seems to contain many non-valid values. `pres_adjusted` seems to differ very little from `PRES`. We will check both variables: if no valid pH_adjusted values are available, and the difference between pressure and pressure_adjusted is very small (in the order of 1e-5 dbar) we can remove them from the dataframe. We will also drop `depth (m)`, since we only have values coming from EMODnet, but we have `PRES` values for all the data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e874878",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "valid_adph=df.shape[0]-sum(df['ph_in_situ_total_adjusted']>999.)-sum(df['ph_in_situ_total_adjusted'].isnull())\n",
    "print('There are', str(valid_adph),\n",
    "      'valid values of ph_in_situ_total_adjusted')\n",
    "if valid_adph == 0:\n",
    "    df.drop(columns=['ph_in_situ_total_adjusted', 'ph_in_situ_total_adjusted_qc'], inplace=True)\n",
    "\n",
    "print('The average offset between pressure an pressure_adjusted is', \n",
    "      str(np.mean(df[vardict['pres']]-df['pres_adjusted (decibar)'])), \n",
    "      'and the standard deviation is',\n",
    "      str(np.std(df[vardict['pres']]-df['pres_adjusted (decibar)'])))\n",
    "\n",
    "if((abs(np.mean(df[vardict['pres']]-df['pres_adjusted (decibar)'])) < 0.00001) &\n",
    "  (abs(np.std(df[vardict['pres']]-df['pres_adjusted (decibar)']) < 0.00001))):\n",
    "    df.drop(columns=['pres_adjusted (decibar)'], inplace=True)\n",
    "    \n",
    "df.drop(columns=['depth (m)'], inplace=True)\n",
    "\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dfec16",
   "metadata": {},
   "source": [
    "## Plotting data: map and position-depth / time series-depth\n",
    "Let's visualize the data we fetched. First we'll plot a world map, color-coded by data source, and a longitude-depth scatter plot to identify which cluster of measurements they correspond to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f36f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "proj=ccrs.cartopy.crs.Miller()\n",
    "plt.figure(dpi=200)\n",
    "ax = plt.axes(projection=proj)\n",
    "\n",
    "#maxlon=max(df_emodnet[vardict['lon']].max(), df_ifremer[vardict['lon']].max())\n",
    "#maxlat=max(df_emodnet[vardict['lat']].max(), df_ifremer[vardict['lat']].max())\n",
    "#minlon=min(df_emodnet[vardict['lon']].min(), df_ifremer[vardict['lon']].min())\n",
    "#minlat=min(df_emodnet[vardict['lat']].min(), df_ifremer[vardict['lat']].min())\n",
    "\n",
    "if ((df[vardict['lon']].min() > -175) & (df[vardict['lon']].max() < 175) & \n",
    "    (df[vardict['lat']].min() > -85) & (df[vardict['lat']].max() < 85)) :\n",
    "    ax.set_extent([df[vardict['lon']].min()-5,df[vardict['lon']].max()+5,\n",
    "                   df[vardict['lat']].max()+5,df[vardict['lat']].min()-5])\n",
    "\n",
    "ax.stock_img()\n",
    "ax.coastlines()\n",
    "\n",
    "scattercolor=['b','r']\n",
    "scattersize=[20,5]\n",
    "counter=0\n",
    "\n",
    "for source in df.SOURCE.unique():\n",
    "    sc=ax.scatter(df[df.SOURCE==source][vardict['lon']],\n",
    "        df[df.SOURCE==source][vardict['lat']],\n",
    "        c=scattercolor[counter],s=scattersize[counter],\n",
    "        label=source,\n",
    "        transform=ccrs.PlateCarree())\n",
    "    counter=counter+1\n",
    "\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb004314",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[10,6])\n",
    "gs = fig.add_gridspec(2, hspace=0.2)\n",
    "axs = gs.subplots(sharex=True, sharey=True)\n",
    "\n",
    "# Common labels (matplotlib 3.4 and higher)\n",
    "#fig.supylabel('pressure (dbar)')\n",
    "#fig.supxlabel('longitude')\n",
    "\n",
    "counter=0\n",
    "for source in df.SOURCE.unique():\n",
    "    sc=axs[counter].scatter(df[df.SOURCE==source][vardict['lon']],\n",
    "        df[df.SOURCE==source][vardict['pres']],\n",
    "        c=df[df.SOURCE==source][vardict['ph']],s=7,\n",
    "        vmax=df[vardict['ph']].max(),\n",
    "        vmin=df[vardict['ph']].min())  \n",
    "    \n",
    "    axs[counter].title.set_text(source)\n",
    "    \n",
    "    counter=counter+1\n",
    "\n",
    "axs[0].invert_yaxis()\n",
    "plt.xticks(rotation=45);\n",
    "\n",
    "# Colorbar in its own axis\n",
    "cb=fig.colorbar(sc, ax=axs.ravel().tolist())\n",
    "cb.set_label('pH')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db9e9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[10,6])\n",
    "gs = fig.add_gridspec(2, hspace=0.2)\n",
    "axs = gs.subplots(sharex=True, sharey=True)\n",
    "\n",
    "# Common labels (matplotlib 3.4 and higher)\n",
    "#fig.supylabel('pressure (dbar)')\n",
    "#fig.supxlabel('date')\n",
    "\n",
    "counter=0\n",
    "for source in df.SOURCE.unique():\n",
    "    sc=axs[counter].scatter(df[df.SOURCE==source]['dateobject'],\n",
    "        df[df.SOURCE==source][vardict['pres']],\n",
    "        c=df[df.SOURCE==source][vardict['ph']],s=7,\n",
    "        vmax=df[vardict['ph']].max(),\n",
    "        vmin=df[vardict['ph']].min())  \n",
    "    \n",
    "    axs[counter].title.set_text(source)\n",
    "    \n",
    "    counter=counter+1\n",
    "\n",
    "axs[0].invert_yaxis()\n",
    "plt.xticks(rotation=45);\n",
    "\n",
    "# Colorbar in its own axis\n",
    "cb=fig.colorbar(sc, ax=axs.ravel().tolist())\n",
    "cb.set_label('pH')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15dfc70",
   "metadata": {},
   "source": [
    "The source datasets partially overlap: EMODnet gathers marine data from various sources; it is very likely that BGC-Argo is one of them. So let's quickly check the metadata.\n",
    "\n",
    "Here we will find how many data points do not have a WMO number associated, how many different platforms are in the dataset, grouped by platform type, and which devices they are.\n",
    "BGC-Argo `platform_type` gives the same information as Ocean-OPS, so there is no need to query the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28d3493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of data points without WMO code\n",
    "print(\"There are\", str(sum(df_emodnet[vardict['wmo']].isnull())), \"datapoints without a WMO number associated in the EMODNet dataframe\")\n",
    "print(\"There are\", str(sum(df_ifremer[vardict['wmo']].isnull())), \"datapoints without a WMO number associated in the BGC-Argo dataframe\")\n",
    "\n",
    "# Retrieve device types\n",
    "wmobytype=df.groupby([df.EP_PLATFORM_TYPE, df.SOURCE, df[vardict['wmo']],df.data_assembly_center], as_index=False).agg({vardict['ph']:['size','mean']})\n",
    "display(wmobytype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752c71cb",
   "metadata": {},
   "source": [
    "We see that the Profiling float data, though differing in length, come from the same individual profile, same data center, and per the plot above, same area and time. There are now two options: \n",
    "\n",
    "**a)** assume (safely) that the best and most updated measurements of Argo floats come from BGC-Argo (IFREMER) and drop the EMODnet values completely, \n",
    "\n",
    "**b)** proceed in the search for duplicates. \n",
    "\n",
    "We will save a csv file with the option a) and explore option b) as far as it's feasible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597ff065",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bgcargo=df.copy()\n",
    "df_bgcargo.drop(columns='dateobject', inplace=True)\n",
    "df_bgcargo=df_bgcargo.loc[~((df_bgcargo['EP_PLATFORM_TYPE'] == 'PF') &\n",
    "                     (df_bgcargo['SOURCE'] == 'EMODnet pH profiles'))]\n",
    "\n",
    "wmobytype=df_bgcargo.groupby(['EP_PLATFORM_TYPE', 'SOURCE', vardict['wmo'],'data_assembly_center'], as_index=False).agg({vardict['ph']:['size','mean']})\n",
    "display(wmobytype)\n",
    "display(df_bgcargo[-1-10:-1])\n",
    "\n",
    "print(\"The resulting dataframe has dimensions\", df_bgcargo.shape)\n",
    "\n",
    "dateforfile=datetime.now().strftime(\"%Y%m%d\")\n",
    "df_bgcargo.to_csv(dateforfile+'_pH_data_ERDDAP_combined_PFonlyBGCArgoIFREMER.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813cbae4",
   "metadata": {},
   "source": [
    "Let's try to identify how many exact duplicates there are in the dataframe by using the `duplicated` method\n",
    " drop them and plot (zooming in)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ceda11",
   "metadata": {},
   "outputs": [],
   "source": [
    "isdup=df.duplicated(subset=[vardict['wmo'],vardict['lat'],vardict['lon'],'dateobject',vardict['pres'],vardict['ph']], \n",
    "                    keep=False)\n",
    "print(str(sum(isdup)),\"rows are full duplicates (pH values included)\")\n",
    "\n",
    "df.drop_duplicates(subset=[vardict['wmo'],vardict['lat'],vardict['lon'],'dateobject',vardict['pres'],vardict['ph']],keep=\"first\", inplace=True, ignore_index=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[12,6])\n",
    "colors = {'BGC-Argo (IFREMER)':'red', 'EMODnet pH profiles':'blue'}\n",
    "sizes={'BGC-Argo (IFREMER)':5,'EMODnet pH profiles':5}\n",
    "ax.scatter(pd.to_datetime(df[vardict['datevec']]), df[vardict['pres']], c=df['SOURCE'].map(colors), s=df['SOURCE'].map(sizes))\n",
    "plt.ylabel('pressure (dbar)')\n",
    "ax.invert_yaxis();\n",
    "plt.xticks(rotation=45);\n",
    "plt.xlim(pd.to_datetime('2017-12-01'), pd.to_datetime('2018-01-01'))\n",
    "plt.ylim(0,10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2848bf35",
   "metadata": {},
   "source": [
    "Many duplicates seem to remain, in very close proximity. Let's check visually the data values from the profiling float; the duplicates may not be exact, but approximate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1700731e",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.loc[df[vardict['wmo']]=='6902879',[vardict['wmo'],vardict['lat'],vardict['lon'],'dateobject',vardict['pres'],vardict['ph'],'SOURCE']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a21fcc",
   "metadata": {},
   "source": [
    "It seems that the differences come down to the significant digits, so we will round the values, including the timestamps to the closest minute, and compare again. We will make a copy first of the dataframe (we might want to keep all the significant digits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139e63ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unrounded=df.copy() # Preserve a copy of the original\n",
    "\n",
    "df=df.round({vardict['ph']: 3, vardict['pres']: 0,vardict['lat']:5,vardict['lon']:5})\n",
    "df.sort_values(by=['dateobject',vardict['pres']],inplace=True)\n",
    "\n",
    "isdup=df.duplicated(subset=[vardict['wmo'],vardict['lat'],vardict['lon'],'dateobject',vardict['pres'],vardict['ph']], \n",
    "                    keep=False)\n",
    "print(str(sum(isdup)),\"rows are full duplicates after rounding\")\n",
    "\n",
    "df.drop_duplicates(subset=[vardict['wmo'],vardict['lat'],vardict['lon'],'dateobject',vardict['pres'],vardict['ph']],keep=\"first\", inplace=True, ignore_index=True)\n",
    "\n",
    "print(\"The dataframe without duplicates has the dimensions\",df.shape,\"vs\",df_unrounded.shape)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[12,6])\n",
    "colors = {'BGC-Argo (IFREMER)':'red', 'EMODnet pH profiles':'blue'}\n",
    "sizes={'BGC-Argo (IFREMER)':5,'EMODnet pH profiles':5}\n",
    "ax.scatter(pd.to_datetime(df[vardict['datevec']]), df[vardict['pres']], c=df['SOURCE'].map(colors), s=df['SOURCE'].map(sizes))\n",
    "plt.ylabel('pressure (dbar)')\n",
    "ax.invert_yaxis();\n",
    "plt.xticks(rotation=45);\n",
    "plt.xlim(pd.to_datetime('2017-12-01'), pd.to_datetime('2018-01-01'))\n",
    "plt.ylim(0,10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7130ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, sharex=True, sharey=False, figsize=[16,8])\n",
    "\n",
    "colors = {'BGC-Argo (IFREMER)':'red', 'EMODnet pH profiles':'blue'}\n",
    "sizes={'BGC-Argo (IFREMER)':5,'EMODnet pH profiles':5}\n",
    "\n",
    "# Common labels (matplotlib 3.4 and higher)\n",
    "#fig.supylabel('pressure (dbar)')\n",
    "#fig.supxlabel('date')\n",
    "\n",
    "axs[0,0].scatter(pd.to_datetime(df[vardict['datevec']]), df[vardict['pres']], c=df['SOURCE'].map(colors), s=df['SOURCE'].map(sizes))\n",
    "axs[0,0].set_xlim(pd.to_datetime('2017-12-01'), pd.to_datetime('2018-01-01'))\n",
    "axs[0,0].invert_yaxis()\n",
    "axs[0,0].title.set_text('Rounded values, duplicates removed')\n",
    "\n",
    "axs[1,0].scatter(pd.to_datetime(df[vardict['datevec']]), df[vardict['pres']], c=df['SOURCE'].map(colors), s=df['SOURCE'].map(sizes))\n",
    "axs[1,0].set_xlim(pd.to_datetime('2017-12-01'), pd.to_datetime('2018-01-01'))\n",
    "axs[1,0].set_ylim(0,10)\n",
    "axs[1,0].invert_yaxis()\n",
    "plt.setp(axs[1,0].get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "axs[0,1].scatter(pd.to_datetime(df_unrounded[vardict['datevec']]), df_unrounded[vardict['pres']], c=df_unrounded['SOURCE'].map(colors), s=df_unrounded['SOURCE'].map(sizes))\n",
    "axs[0,1].set_xlim(pd.to_datetime('2017-12-01'), pd.to_datetime('2018-01-01'))\n",
    "axs[0,1].invert_yaxis()\n",
    "axs[0,1].title.set_text('Not-rounded, with duplicates')\n",
    "\n",
    "axs[1,1].scatter(pd.to_datetime(df_unrounded[vardict['datevec']]), df_unrounded[vardict['pres']], c=df_unrounded['SOURCE'].map(colors), s=df_unrounded['SOURCE'].map(sizes))\n",
    "axs[1,1].set_xlim(pd.to_datetime('2017-12-01'), pd.to_datetime('2018-01-01'))\n",
    "axs[1,1].set_ylim(0,10)\n",
    "axs[1,1].invert_yaxis()\n",
    "plt.setp(axs[1,1].get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71eb36ea",
   "metadata": {},
   "source": [
    "So far, we have identified and eliminated duplicates, and we can export to a csv file, for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c19b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "dateforfile=datetime.now().strftime(\"%Y%m%d\")\n",
    "df.to_csv(dateforfile+'_pH_data_ERDDAP_combined_removeduplicates.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a1c214",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
